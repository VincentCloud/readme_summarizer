{
 "cells": [
  {
   "source": [
    "# Preprocess the data"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfm\n",
    "import pandas as pd\n",
    "import string\n",
    "import logging\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from interruptingcow import timeout\n",
    "\n",
    "COUNT = 0\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, data_path, filename, extractive=False):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.ESCAPES = ''.join([chr(char) for char in range(1, 32)])\n",
    "        self.filename = filename\n",
    "        self.COUNT = 0\n",
    "        self.faulty_indices = []\n",
    "        self.extractive = extractive\n",
    "\n",
    "    def remove_starting_periods(self, readme: str):\n",
    "        if readme.startswith('.'):\n",
    "            print('removing starting periods...')\n",
    "            while readme.startswith('.'):\n",
    "                readme = readme.replace('.', '', 1)\n",
    "        return readme\n",
    "\n",
    "    def bulk_remove_starting_periods(self, column_to_clean='readme'):\n",
    "        self.df[column_to_clean] = self.df[column_to_clean].apply(self.remove_starting_periods)\n",
    "\n",
    "    def remove_punctuation_and_escapes(self, column_to_clean='readme'):\n",
    "        print('Removing punctuations...')\n",
    "        if self.extractive:\n",
    "            punctuation = string.punctuation.replace('.', '')\n",
    "        else:\n",
    "            punctuation = string.punctuation\n",
    "        for i in range(len(self.df[column_to_clean])):\n",
    "            self.df[column_to_clean][i] = self.df[column_to_clean][i] \\\n",
    "                .translate(str.maketrans(self.ESCAPES, ' ' * len(self.ESCAPES))) \\\n",
    "                .translate(str.maketrans('', '', punctuation))\n",
    "        self.bulk_remove_starting_periods()\n",
    "\n",
    "    def findCodeBlocks(self, githubFlavoredMarkdown):\n",
    "            self.COUNT += 1\n",
    "            return re.findall(r'```(?:[^`]+|`(?!``))*```', githubFlavoredMarkdown)\n",
    "\n",
    "    def delete_code_blocks(self, readme):\n",
    "        codes = self.findCodeBlocks(readme)\n",
    "        print(f'Deleting code blocks for index {self.COUNT}')\n",
    "        for code in codes:\n",
    "            readme = readme.replace(code, \"\")\n",
    "        return readme\n",
    "\n",
    "    def delete_code_blocks2(self, readme):\n",
    "        substituted_text = re.sub(r'```.*?```', \"\", readme, re.MULTILINE, re.DOTALL)\n",
    "        substituted_text = re.sub(r'`.*?`', \"\", substituted_text, re.MULTILINE, re.DOTALL)\n",
    "        return substituted_text\n",
    "\n",
    "    def to_lower_case(self, readme):\n",
    "        return readme.lower()\n",
    "\n",
    "    def bulk_to_lower_case(self, column_to_clean='readme'):\n",
    "        print('Converting to lower cases')\n",
    "        self.df[column_to_clean] = self.df[column_to_clean].apply(self.to_lower_case)\n",
    "\n",
    "    def bulk_delete_code_blocks(self, column_to_clean='readme'):\n",
    "        print('Deleting code blocks...')\n",
    "        for i in range(len(self.df[column_to_clean])):\n",
    "            try:\n",
    "                with timeout(2, exception=RuntimeError):\n",
    "                    self.df[column_to_clean][i] = self.delete_code_blocks2(self.df[column_to_clean][i])\n",
    "            except RuntimeError:\n",
    "                print('timeout deleting code blocks')\n",
    "                self.faulty_indices.append(i)\n",
    "\n",
    "    def write_faulty_index_to_txt_file(self):\n",
    "        print('writing faulty indices to txt files...')\n",
    "        with open(f'/Users/vincenthuang/Development/Summer-2020/readme_summarizer/data/cleaned_data/incidents'\n",
    "                  f'/faulty_indices_{self.filename}.txt', 'w+') as fn:\n",
    "            fn.write(str(self.faulty_indices))\n",
    "\n",
    "    def delete_redundent_spaces(self, readme):\n",
    "        readme = re.sub('\\t+', ' ', readme)\n",
    "        return re.sub(' +', ' ', readme)\n",
    "\n",
    "    def bulk_delete_redundent_spaces(self, column_to_clean='readme'):\n",
    "        print('deleting redundent spaces...')\n",
    "        self.df['readme'] = self.df[column_to_clean].apply(self.delete_redundent_spaces)\n",
    "\n",
    "    def convert_to_html(self, readme):\n",
    "        return gfm.markdown(readme)\n",
    "\n",
    "    def get_text_from_html(self, html):\n",
    "        soup = BeautifulSoup(html)\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        return text\n",
    "\n",
    "    def remove_markdown(self, readme):\n",
    "        self.COUNT += 1\n",
    "        html = self.convert_to_html(readme)\n",
    "        readme = self.get_text_from_html(html)\n",
    "        return readme\n",
    "\n",
    "    def bulk_remove_markdown(self, column_to_clean='readme'):\n",
    "        self.COUNT = 0\n",
    "        for i in range(len(self.df[column_to_clean])):\n",
    "            try:\n",
    "                with timeout(2, exception=RuntimeError):\n",
    "                    self.df[column_to_clean][i] = self.remove_markdown(self.df[column_to_clean][i])\n",
    "            except RuntimeError:\n",
    "                print('timeout removing markdown')\n",
    "                self.faulty_indices.append(i)\n",
    "\n",
    "    def remove_faulty_rows(self):\n",
    "        print('removing faulty rows...')\n",
    "        self.df.drop(self.faulty_indices, inplace=True)\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "    def lemmatize(self, readme):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        punctuations=string.punctuation\n",
    "        words = nltk.word_tokenize(readme)\n",
    "        for word in words:\n",
    "            if word in punctuations:\n",
    "                words.remove(word)\n",
    "        for i in range(len(words)):\n",
    "            words[i] = wordnet_lemmatizer.lemmatize(words[i], get_wordnet_pos(words[i]))\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "    def bulk_lemmatize(self, column_to_clean='readme'):\n",
    "        self.df[column_to_clean] = self.df[column_to_clean].apply(lemmatize)\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, column_to_clean='readme'):\n",
    "        print('preprocessing data...')\n",
    "        try:\n",
    "            self.bulk_delete_code_blocks() # delete code blocks\n",
    "            self.bulk_remove_markdown() # remove markdown\n",
    "            self.remove_punctuation_and_escapes() # remove punctuations and escape characters\n",
    "            self.bulk_delete_redundent_spaces() # delete redundent spaces\n",
    "            self.bulk_to_lower_case() # convert to lower case\n",
    "            self.write_faulty_index_to_txt_file()\n",
    "            self.remove_faulty_rows()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logging.error(FileNotFoundError)\n",
    "\n",
    "    def export(self):\n",
    "        print('exporting...')\n",
    "        if self.extractive:\n",
    "            self.df.to_csv(\n",
    "                f'..\\cleaned_data_extractive\\{self.filename}'\n",
    "            )\n",
    "        else:\n",
    "            self.df.to_csv(\n",
    "                f'..\\cleaned_data\\{self.filename}'\n",
    "            )\n",
    "\n",
    "    def report(self):\n",
    "        print(self.faulty_indices)\n",
    "        return self.faulty_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_preprocessor = Preprocessor(\n",
    "        'C:\\Users\\chuang77\\Documents\\GitHub\\readme_summarizer\\data\\original_data\\train.readme_data_with_purpose.csv',\n",
    "        'train.cleaned_readme_data_with_purpose.csv',\n",
    "        extractive=False\n",
    "    )\n",
    "    eval_preprocessor = Preprocessor(\n",
    "        'C:\\Users\\chuang77\\Documents\\GitHub\\readme_summarizer\\data\\original_data\\valid.readme_data_with_purpose.csv',\n",
    "        'valid.cleaned_readme_data_with_purpose.csv',\n",
    "        extractive=False\n",
    "    )\n",
    "    test_preprocessor = Preprocessor(\n",
    "        'C:\\Users\\chuang77\\Documents\\GitHub\\readme_summarizer\\data\\original_data\\test.readme_data_with_purpose.csv',\n",
    "        'test.cleaned_readme_data_with_purpose.csv',\n",
    "        extractive=False\n",
    "    )\n",
    "\n",
    "    preprocessors = [train_preprocessor, eval_preprocessor, test_preprocessor]\n",
    "\n",
    "    for preprocessor in preprocessors:\n",
    "        preprocessor.preprocess()\n",
    "        preprocessor.report()\n",
    "        preprocessor.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You be bad he be bad John play the guitar in the shadow\n"
    }
   ],
   "source": [
    "# test for nltk lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization(document):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    punctuations=string.punctuation\n",
    "    words = nltk.word_tokenize(document)\n",
    "    for word in words:\n",
    "        if word in punctuations:\n",
    "            words.remove(word)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = wordnet_lemmatizer.lemmatize(words[i], get_wordnet_pos(words[i]))\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'This is an library'"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "sent = \"This is an npm library\"\n",
    "\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "         if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bitmlvenvfa475b1bb51b4a348a25083dba945238",
   "display_name": "Python 3.8.3 32-bit ('ml': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}