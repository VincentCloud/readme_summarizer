knowledgedistillationpytorch exploring knowledge distillation of dnns for efficient hardware solutions author haitong li framework pytorch dataset cifar10 features a framework for exploring shallow and deep knowledge distillation kd experiments hyperparameters defined by params.json universally avoiding long argparser commands hyperparameter searching and result synthesizing as a table progress bar tensorboard support and checkpoint savingloading utils.py pretrained teacher models available for download install clone the repo install the dependencies including pytorch organizatoin .train.py main entrance for traineval with or without kd on cifar10 .experiments json files for each experiment dir for hypersearch .model teacher and student dnns knowledge distillation kd loss defination dataloader key notes about usage for your experiments download the zip file for pretrained teacher model checkpoints from this box folder simply move the unzipped subfolders into knowledgedistillationpytorchexperiments replacing the existing ones if necessary follow the default path naming call train.py to start training 5layer cnn with resnet18s dark knowledge or training resnet18 with stateoftheart deeper models distilled use searchhyperparams.py for hypersearch hyperparameters are defined in params.json files universally.